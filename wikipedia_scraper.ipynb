{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48qvKH56a1Wy"
      },
      "source": [
        "## API Scraping\n",
        "\n",
        "### A simple API query\n",
        "You will start with the basics: how to do a simple request to an [API endpoint](../../2.python/2.python_advanced/05.Scraping/5.apis.ipynb).\n",
        "\n",
        "You will use the [requests](https://requests.readthedocs.io/en/latest/) external library through the `import` keyword.\n",
        "\n",
        "NOTE: external libraries need to be installed first. Check their documentation.\n",
        "\n",
        "Check the [Quickstart](https://requests.readthedocs.io/en/latest/user/quickstart/) section of the Requests documentation to:\n",
        "1. use the `get()` method to connect to this endpoint: https://country-leaders.onrender.com/status\n",
        "2. check if the `status_code` is equal to 200, which means OK.\n",
        "    * if OK, `print()` the `text`` of the response.\n",
        "    * if not, `print()` the `status_code`. \n",
        "\n",
        "Here is the signification of [HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gIEFBhBeAkzf"
      },
      "outputs": [],
      "source": [
        "# import the requests library (1 line)\n",
        "# assign the root url (without /status) to the root_url variable for ease of reference (1 line)\n",
        "\n",
        "# assign the /status endpoint to another variable called status_url (1 line)\n",
        "\n",
        "# query the /status endpoint using the get() method and store it in the req variable (1 line)\n",
        "\n",
        "# check the status_code using a condition and print appropriate messages (4 lines)\n",
        "\n",
        "# assign the output to the leaders variable (1 line)\n",
        "\n",
        "# display the leaders variable (1 line)\n",
        "\n",
        "# does it work?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x25JA6vaRBi"
      },
      "source": [
        "### Cookies anyone?\n",
        "\n",
        "It looks like the access to this API is restricted...\n",
        "Query the `/cookie` endpoint and extract the appropriate field to access your cookie.\n",
        "\n",
        "You will need to use this cookie in each of the following API requests : `/countries`, `/leaders`, `/leader`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHBNaFJo2M9e"
      },
      "source": [
        "Try to query the countries endpoint using the cookie, save the output and print it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get cookie from api\n",
        "\n",
        "#try get coutnry from api passing cookies argument "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1Fc1mHySn9g"
      },
      "source": [
        "## Extracting data from Wikipedia\n",
        "\n",
        "Query one of the leaders Wikipedia urls (from api `/leader` endpoint) and display its `text` (not JSON)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cEKKqyTHr3fD"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlsqjiTYr8sK"
      },
      "source": [
        "Ouch! You get the raw HTML code of the webpage. If you try to deal with it without tools, you will be there all night. Instead, use the [beautiful soup 4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) *external* library. You will find more info about it [here](../../2.python/2.python_advanced/05.Scraping/1.beautifulsoup_basic.ipynb) and [here](../../2.python/2.python_advanced/05.Scraping/2.beautifulsoup_advanced.ipynb)\n",
        "\n",
        "Using the Quickstart section, start by importing the library and loading the output of your `get_text()` function.\n",
        "\n",
        "Use the `prettify()` function and print it to take a look. You will start the actual parsing in the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "h79ahwJvr7p-"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQsLjaig7_dY"
      },
      "source": [
        "That looks better but you need to extract the right part of the webpage: the text of the first paragraph.\n",
        "\n",
        "It is a bit tricky because Wikipedia pages slightly differ in structure from one language to the next. We cannot simply get the text for the first HTML paragraph.\n",
        "\n",
        "You will start by getting all the HTML paragraphs from the HTML source and saving them in the `paragraphs` variable.\n",
        "\n",
        "Use the documentation or google the appropriate keywords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Vs8HeBx19oyC"
      },
      "outputs": [],
      "source": [
        "# 2 lines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tlaL3aM9zoo"
      },
      "source": [
        "If you try different urls, you might find that the paragraph you want may be at a different index each time.\n",
        "\n",
        "That is where you need to be clever and ask yourself what would be a reliable way to identify the right index ie. which string matches only the first paragraph whatever the language...\n",
        "\n",
        "Spend a good 30 minutes on the problem and brainstorm with your fellow learners. If you come out empty handed, ask your coach.\n",
        "\n",
        "1. Loop over the HTML paragraphs\n",
        "2. When you have identified the correct one\n",
        "  * store the [text](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#output) inside the `first_paragraph` variable\n",
        "  * exit the loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0DduDXaQALau"
      },
      "outputs": [],
      "source": [
        "# <10 lines\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFe-1LlIHBGm"
      },
      "source": [
        "At this stage, you can create a function to maintain consistency in your code. We will give you its *skeleton*, you will copy the code you wrote and make it work inside a function.\n",
        "\n",
        "Don't forget to test your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wQORoweDHARO"
      },
      "outputs": [],
      "source": [
        "# 10 lines\n",
        "# def get_first_paragraph(wikipedia_url):\n",
        "#   print(wikipedia_url) # keep this for the rest of the notebook\n",
        "#   [insert your code]\n",
        "#   return first_paragraph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtoM4dgsBVoD"
      },
      "source": [
        "### Regular expressions to the rescue\n",
        "\n",
        "Now that you have extracted the content of the first paragraph, the only thing that remains to finish your Wikipedia scraper is to sanitize the output.\n",
        "\n",
        "Indeed some Wikipedia references, HTML code, phonetic pronunciation etc. may linger. You might find *regular expressions* handy to get rid of them and obtain pristine text. You will find some useful documentation about regular expressions [here](../../2.python/2.python_advanced/03.Regex/regex.ipynb)\n",
        "\n",
        "Once you have one of your regex working online, try it in the cell below. \n",
        "\n",
        "Hints: \n",
        "* Check the `sub()` method documentation.\n",
        "* Make sure to test urls in different languages. Some may look good but other do not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7DHEAb6oBUxd"
      },
      "outputs": [],
      "source": [
        "# 3 lines\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
